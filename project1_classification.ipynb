{"cells": [{"cell_type": "code", "execution_count": null, "id": "94d3b18e", "metadata": {"collapsed": true, "deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs236299-2023-spring/project1.git .tmp\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "id": "d5027944", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "raw", "id": "2bfe698f", "metadata": {"jupyter": {"source_hidden": true}}, "source": ["%%latex\n", "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\newcommand{\\softmax}{\\operatorname{softmax}}\n", "\\newcommand{\\Prob}{\\Pr}\n", "\\newcommand{\\given}{\\,|\\,}"]}, {"cell_type": "markdown", "id": "2563869b", "metadata": {}, "source": ["$$\n", "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n", "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n", "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n", "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n", "\\renewcommand{\\Prob}{\\Pr}\n", "\\renewcommand{\\given}{\\,|\\,}\n", "$$"]}, {"attachments": {}, "cell_type": "markdown", "id": "8f82eb20", "metadata": {"colab_type": "text", "id": "JyMdovPAKAHU", "tags": ["remove_for_latex"]}, "source": ["# Course 236299\n", "\n", "## Project segment 1: Text classification\n", "\n", "In this project segment you will build several varieties of text classifiers using PyTorch.\n", "\n", "1. A majority baseline.\n", "2. A naive Bayes classifer.\n", "3. A logistic regression (single-layer perceptron) classifier.\n", "4. A multilayer perceptron classifier."]}, {"attachments": {}, "cell_type": "markdown", "id": "82ba2a8f", "metadata": {}, "source": ["# Preparation"]}, {"cell_type": "code", "execution_count": null, "id": "f575f075", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["import copy\n", "import re\n", "import wget\n", "import csv\n", "import torch\n", "import torch.nn as nn\n", "import datasets\n", "\n", "from datasets import load_dataset\n", "from tokenizers import Tokenizer\n", "from tokenizers.pre_tokenizers import Whitespace\n", "from tokenizers import normalizers\n", "from tokenizers.models import WordLevel\n", "from tokenizers.trainers import WordLevelTrainer\n", "from transformers import PreTrainedTokenizerFast\n", "from collections import Counter\n", "from torch import optim\n", "from tqdm.auto import tqdm"]}, {"cell_type": "code", "execution_count": null, "id": "d1a92710", "metadata": {"colab": {}, "colab_type": "code", "id": "kdi-spgB0sEi"}, "outputs": [], "source": ["# Random seed\n", "random_seed = 1234\n", "torch.manual_seed(random_seed)\n", "\n", "## GPU check\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(device)"]}, {"cell_type": "markdown", "id": "b0e162e4", "metadata": {"colab_type": "text", "id": "drbeoB66kJLd"}, "source": ["# The task: Answer types for ATIS queries\n", "\n", "For this and future project segments, you will be working with a standard natural-language-processing dataset, the [ATIS (Airline Travel Information System) dataset](https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk). This dataset is composed of queries about flights \u2013 their dates, times, locations, airlines, and the like.\n", "\n", "Over the years, the dataset has been annotated in all kinds of ways, with parts of speech, informational chunks, parse trees, and even corresponding SQL database queries. You'll use various of these annotations in future assignments. For this project segment, however, you'll pursue an easier classification task: **given a query, predict the answer type**.\n", "\n", "These queries ask for different types of answers, such as\n", "\n", "* Flight IDs: \"Show me the flights from Washington to Boston\"\n", "* Fares: \"How much is the cheapest flight to Milwaukee\"\n", "* City names: \"Where does flight 100 fly to?\"\n", "\n", "In all, there are some 30 answer types to the queries.\n", "\n", "Below is an example taken from this dataset:\n", "\n", "_Query:_\n", "\n", "```\n", "show me the afternoon flights from washington to boston\n", "```\n", "\n", "_SQL:_\n", "\n", "```\n", "SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport_service airport_service_1 , city city_1 , airport_service airport_service_2 , city city_2 \n", "   WHERE flight_1.departure_time BETWEEN 1200 AND 1800 \n", "     AND ( flight_1.from_airport = airport_service_1.airport_code \n", "           AND airport_service_1.city_code = city_1.city_code \n", "           AND city_1.city_name = 'WASHINGTON' \n", "           AND flight_1.to_airport = airport_service_2.airport_code \n", "           AND airport_service_2.city_code = city_2.city_code \n", "           AND city_2.city_name = 'BOSTON' )\n", "```\n", "\n", "In this project segment, we will consider the answer type for a natural-language query to be the target field of the corresponding SQL query. For the above example, the answer type would be *flight_id*."]}, {"cell_type": "markdown", "id": "193b2abd", "metadata": {"colab_type": "text", "id": "drbeoB66kJLd"}, "source": ["## Loading and preprocessing the data\n", "\n", "> Read over this section, executing the cells, and **making sure you understand what's going on before proceeding to the next parts.**\n", "\n", "First, let's download the dataset."]}, {"cell_type": "code", "execution_count": null, "id": "3f47d516", "metadata": {"colab": {}, "colab_type": "code", "id": "zGVWcvlk080Q"}, "outputs": [], "source": ["data_dir = \"https://raw.githubusercontent.com/nlp-236299/data/master/ATIS/\"\n", "os.makedirs('data', exist_ok=True)\n", "for split in ['train', 'dev', 'test']:\n", "    wget.download(f\"{data_dir}/{split}.nl\", out='data/')\n", "    wget.download(f\"{data_dir}/{split}.sql\", out='data/')"]}, {"cell_type": "markdown", "id": "6b50b7d3", "metadata": {}, "source": ["Next, we process the dataset by extracting answer types from SQL queries and saving in CSV format."]}, {"cell_type": "code", "execution_count": null, "id": "064ca5f1", "metadata": {}, "outputs": [], "source": ["def get_label_from_query(query):\n", "    \"\"\"Returns the answer type from `query` by dead reckoning.\n", "    It's basically the second or third token in the SQL query.\n", "    \"\"\"    \n", "    match = re.match(r'\\s*SELECT\\s+(DISTINCT\\s*)?(\\w+\\.)?(?P<label>\\w+)', query)\n", "    if match:\n", "        label = match.group('label')\n", "    else:\n", "        raise RuntimeError(f'no label in query {query}')\n", "    return label\n", "\n", "for split in ['train', 'dev', 'test']:\n", "    sql_file = f'data/{split}.sql'\n", "    nl_file = f'data/{split}.nl'\n", "    out_file = f'data/{split}.csv'\n", "    \n", "    with open(nl_file) as f_nl:\n", "        with open(sql_file) as f_sql:\n", "            with open(out_file, 'w') as fout:\n", "                writer = csv.writer(fout)\n", "                writer.writerow(('label','text'))\n", "                for text, sql in zip(f_nl, f_sql):\n", "                    text = text.strip()\n", "                    sql = sql.strip()\n", "                    label = get_label_from_query(sql)\n", "                    writer.writerow((label, text))"]}, {"attachments": {}, "cell_type": "markdown", "id": "4db1eb37", "metadata": {}, "source": ["Let's take a look at what the data file looks like."]}, {"cell_type": "code", "execution_count": null, "id": "6d49c307", "metadata": {}, "outputs": [], "source": ["shell('head \"data/train.csv\"')"]}, {"cell_type": "markdown", "id": "d20e26e8", "metadata": {}, "source": ["We use `datasets` to prepare the data, as in lab 1-5. More information on `datasets` can be found at [https://huggingface.co/docs/datasets/loading](https://huggingface.co/docs/datasets/loading)."]}, {"cell_type": "code", "execution_count": null, "id": "a19aa89e", "metadata": {}, "outputs": [], "source": ["atis = load_dataset('csv', data_files={'train':'data/train.csv', \\\n", "                                       'val': 'data/dev.csv', \\\n", "                                       'test': 'data/test.csv'})"]}, {"cell_type": "code", "execution_count": null, "id": "f8206b76", "metadata": {}, "outputs": [], "source": ["atis"]}, {"cell_type": "code", "execution_count": null, "id": "04d4310a", "metadata": {}, "outputs": [], "source": ["train_data = atis['train']\n", "val_data = atis['val']\n", "test_data = atis['test']\n", "\n", "train_data.shuffle(seed=random_seed)"]}, {"cell_type": "markdown", "id": "a03dc6d5", "metadata": {}, "source": ["We build a tokenizer from the training data to tokenize text and convert tokens into word ids."]}, {"cell_type": "code", "execution_count": null, "id": "cee1cbb3", "metadata": {}, "outputs": [], "source": ["MIN_FREQ = 3 # words appearing fewer than 3 times are treated as 'unknown'\n", "unk_token = '[UNK]'\n", "pad_token = '[PAD]'\n", "\n", "tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n", "tokenizer.pre_tokenizer = Whitespace()\n", "tokenizer.normalizer = normalizers.Lowercase()\n", "\n", "trainer = WordLevelTrainer(min_frequency=MIN_FREQ, special_tokens=[pad_token, unk_token])\n", "tokenizer.train_from_iterator(train_data['text'], trainer=trainer)"]}, {"cell_type": "markdown", "id": "eb44f8c1", "metadata": {}, "source": ["We use `datasets.Dataset.map` to convert text into word ids. As shown in lab 1-5, first we need to wrap `tokenizer` with the `transformers.PreTrainedTokenizerFast` class to be compatible with the `datasets` library."]}, {"cell_type": "code", "execution_count": null, "id": "f24daa66", "metadata": {}, "outputs": [], "source": ["hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, pad_token=pad_token, unk_token=unk_token)"]}, {"cell_type": "code", "execution_count": null, "id": "7162f8ce", "metadata": {}, "outputs": [], "source": ["def encode(example):\n", "    return hf_tokenizer(example['text'])\n", "\n", "train_data = train_data.map(encode)\n", "val_data = val_data.map(encode)\n", "test_data = test_data.map(encode)"]}, {"cell_type": "markdown", "id": "641434d6", "metadata": {}, "source": ["We also need to convert label strings into label ids."]}, {"cell_type": "code", "execution_count": null, "id": "2eb10575", "metadata": {}, "outputs": [], "source": ["# Add a new column `label_id`\n", "train_data = train_data.add_column('label_id', train_data['label'])\n", "val_data = val_data.add_column('label_id', val_data['label'])\n", "test_data = test_data.add_column('label_id', test_data['label'])\n", "\n", "# Convert feature `label_id` from strings to integer ids\n", "train_data = train_data.class_encode_column('label_id')\n", "\n", "# Use the label vocabulary on training data to convert val and test sets\n", "label2id = train_data.features['label_id']._str2int\n", "val_data = val_data.class_encode_column('label_id')\n", "val_data = val_data.align_labels_with_mapping(label2id, \"label_id\")\n", "test_data = test_data.class_encode_column('label_id')\n", "test_data = test_data.align_labels_with_mapping(label2id, \"label_id\")"]}, {"cell_type": "code", "execution_count": null, "id": "09f9c110", "metadata": {}, "outputs": [], "source": ["# Compute size of vocabulary\n", "text_vocab = tokenizer.get_vocab()\n", "label_vocab = train_data.features['label_id']._str2int\n", "vocab_size = len(text_vocab)\n", "num_labels = len(label_vocab)\n", "print(f\"Size of vocab: {vocab_size}\")\n", "print(f\"Number of labels: {num_labels}\")"]}, {"cell_type": "markdown", "id": "8cc57196", "metadata": {}, "source": ["To get a sense of the kinds of things that are asked about in this dataset, here is the list of all of the answer types in the training data."]}, {"cell_type": "code", "execution_count": null, "id": "5dcc16c1", "metadata": {}, "outputs": [], "source": ["for label in label_vocab:\n", "    print(f\"{label_vocab[label]:2d} {label}\") "]}, {"cell_type": "markdown", "id": "2c724fd7", "metadata": {"colab_type": "text", "id": "5HCgGp4ACIvL"}, "source": ["## Handling unknown words\n", "\n", "Note that we mapped words appearing fewer than 3 times to a special _unknown_ token (we're using `[UNK]`) for two reasons: \n", "\n", "1. Due to the scarcity of such rare words in training data, we might not be able to learn generalizable conclusions about them.\n", "2. Introducing an unknown token allows us to deal with out-of-vocabulary words in the test data as well: we just map those words to `[UNK]`."]}, {"cell_type": "code", "execution_count": null, "id": "bfbc6ccb", "metadata": {"colab": {}, "colab_type": "code", "id": "Tr5Omf6yBTsI"}, "outputs": [], "source": ["print (f\"Unknown token: {unk_token}\")\n", "unk_index = text_vocab[unk_token]\n", "print (f\"Unknown token id: {unk_index}\")\n", "\n", "# UNK example\n", "example_unk_token = 'IAmAnUnknownWordForSure'\n", "print (f\"An unknown token: {example_unk_token}\")\n", "print (f\"Mapped back to word id: {hf_tokenizer(example_unk_token).input_ids}\")\n", "print (f\"Mapped to [UNK]'s?: {all([id == unk_index for id in hf_tokenizer(example_unk_token).input_ids])}\")"]}, {"cell_type": "markdown", "id": "094f121f", "metadata": {}, "source": ["To facilitate batching sentences of different lengths into the same tensor as we'll see later, we also reserved a special padding symbol `[PAD]`."]}, {"cell_type": "code", "execution_count": null, "id": "0731b1da", "metadata": {}, "outputs": [], "source": ["print (f\"Padding token: {pad_token}\")\n", "pad_index = text_vocab[pad_token]\n", "print (f\"Padding token id: {pad_index}\")"]}, {"cell_type": "markdown", "id": "21e601dc", "metadata": {"colab_type": "text", "id": "go2q9-vd6RO7"}, "source": ["## Batching the data\n", "\n", "To load data in batches, we use `torch.utils.data.DataLoader`. This enables us to iterate over the dataset under a given `BATCH_SIZE` which specifies how many examples we want to process at a time."]}, {"cell_type": "code", "execution_count": null, "id": "58a49bc8", "metadata": {}, "outputs": [], "source": ["BATCH_SIZE = 32\n", "\n", "# Defines how to batch a list of examples together\n", "def collate_fn(examples):\n", "    batch = {}\n", "    bsz = len(examples)\n", "    label_ids = []\n", "    for example in examples:\n", "        label_ids.append(example['label_id'])\n", "    label_batch = torch.LongTensor(label_ids).to(device)\n", "    input_ids = []\n", "    for example in examples:\n", "        input_ids.append(example['input_ids'])\n", "    max_length = max([len(word_ids) for word_ids in input_ids])\n", "    text_batch = torch.zeros(bsz, max_length).long().fill_(pad_index).to(device)\n", "    for b in range(bsz):\n", "        text_batch[b][:len(input_ids[b])] = torch.LongTensor(input_ids[b]).to(device)\n", "    \n", "    batch['label_ids'] = label_batch\n", "    batch['input_ids'] = text_batch\n", "    return batch\n", "\n", "train_iter = torch.utils.data.DataLoader(train_data, \n", "                                         batch_size=BATCH_SIZE,\n", "                                         collate_fn=collate_fn)\n", "val_iter = torch.utils.data.DataLoader(val_data, \n", "                                       batch_size=BATCH_SIZE, \n", "                                       collate_fn=collate_fn)\n", "test_iter = torch.utils.data.DataLoader(test_data, \n", "                                        batch_size=BATCH_SIZE, \n", "                                        collate_fn=collate_fn)"]}, {"cell_type": "markdown", "id": "f2667709", "metadata": {}, "source": ["Let's look at a single batch from one of these iterators."]}, {"cell_type": "code", "execution_count": null, "id": "9de1f906", "metadata": {}, "outputs": [], "source": ["batch = next(iter(train_iter))\n", "text = batch['input_ids']\n", "print (f\"Size of text batch: {text.size()}\")\n", "print (f\"Third sentence in batch: {text[2]}\")\n", "print (f\"Mapped back to string: {hf_tokenizer.decode(text[2])}\")\n", "print (f\"Mapped back to string skipping padding: {hf_tokenizer.decode(text[2], skip_special_tokens=True)}\")\n", "\n", "label = batch['label_ids']\n", "label_vocab_itos = train_data.features['label_id']._int2str # map from label ids to strs\n", "print (f\"Size of label batch: {label.size()}\")\n", "print (f\"Third label in batch: {label[2]}\")\n", "print (f\"Mapped back to string: {label_vocab_itos[label[2].item()]}\")"]}, {"cell_type": "markdown", "id": "496e57c6", "metadata": {"colab_type": "text", "id": "5iL-spuDoLDt"}, "source": ["You might notice some padding tokens `[PAD]` when we convert word ids back to strings, or equivalently, padding ids `0` in the corresponding tensor. The reason why we need such padding is because the sentences in a batch might be of different lengths, and to save them in a 2D tensor for parallel processing, sentences that are shorter than the longest sentence need to be padded with some placeholder values. Later during training you'll need to make sure that the paddings do not affect the final results."]}, {"cell_type": "markdown", "id": "3ef52e05", "metadata": {"colab_type": "text", "id": "H6rFmJcj-rgl"}, "source": ["Alternatively, we can also directly iterate over the individual examples in `train_data`, `val_data` and `test_data`. Here the returned values are the raw sentences and labels instead of their corresponding ids, and you might need to explicitly deal with the unknown words, unlike using bucket iterators which automatically map unknown words to an unknown word id."]}, {"cell_type": "code", "execution_count": null, "id": "27ead4c9", "metadata": {"colab": {}, "colab_type": "code", "id": "Q2GGBhTF-5p0"}, "outputs": [], "source": ["for _, example in zip(range(5), train_data):\n", "  print(f\"{example['label']:10} -- {example['text']}\")"]}, {"cell_type": "markdown", "id": "1cdbafc8", "metadata": {}, "source": ["## Notations used\n", "\n", "In this project segment, we'll use the following notations.\n", "\n", "* Sequences of elements (vectors and the like) are written with angle brackets and commas ($\\langle w_1, \\ldots, w_M \\rangle$) or directly with no punctuation ($w_1 \\cdots w_M$).\n", "* Sets are notated similarly but with braces, ($\\{ v_1, \\ldots, v_V \\}$).\n", "* Maximum indices ($M$, $N$, $V$, $T$, and $X$ in the following) are written as uppercase italics.\n", "* Variables over sequences and sets are written in boldface ($\\vect{w}$), typically with the same letter as the variables over their elements.\n", "\n", "In particular,\n", "\n", "* $\\vect{w} = w_1 \\cdots w_M$: A text to be classified, each element $w_j$ being a word token.\n", "* $\\vect{v} = \\{ v_1, \\ldots, v_V\\}$: A vocabulary, each element $v_k$ being a word type.\n", "* $\\vect{x} = \\langle x_1, \\ldots, x_X \\rangle$: Input features to a model.\n", "* $\\vect{y} = \\{ y_1, \\ldots, y_N \\}$: The output classes of a model, each element $y_i$ being a class label.\n", "* $\\vect{T} = \\langle \\vect{w}^{(1)}, \\ldots, \\vect{w}^{(T)} \\rangle$: The training corpus of texts.\n", "* $\\vect{Y} = \\langle y^{(1)}, \\ldots, y^{(T)} \\rangle$: The corresponding gold labels for the training examples in $T$."]}, {"cell_type": "markdown", "id": "320375db", "metadata": {"colab_type": "text", "id": "CxDMbHJG9Qpg"}, "source": ["# To Do: Establish a majority baseline\n", "\n", "A simple baseline for classification tasks is to always predict the most common class. \n", "Given a training set of texts $\\vect{T}$ labeled by classes $\\vect{Y}$, we classify an input text $\\vect{w} = w_1 \\cdots w_M$ as the class $y_i$ that occurs most frequently in the training data, that is, specified by\n", "\n", "$$ \\argmax{i} \\cnt{y_i} $$\n", "\n", "and thus ignoring the input entirely (!).\n", "\n", "**Implement the majority baseline and compute test accuracy using the starter code below.** For this baseline, and for the naive Bayes classifier later, we don't need to use the validation set since we don't tune any hyper-parameters."]}, {"cell_type": "code", "execution_count": null, "id": "a391dbd4", "metadata": {"colab": {}, "colab_type": "code", "id": "Hd8XvBof6rVa"}, "outputs": [], "source": ["# TODO\n", "def majority_baseline_accuracy(train_data, test_data):\n", "  \"\"\"Returns the most common label in the training set, and the accuracy of\n", "     the majority baseline on the test set.\n", "  \"\"\"\n", "  ...\n", "  return most_common_label, test_accuracy"]}, {"cell_type": "markdown", "id": "70be71e8", "metadata": {}, "source": ["How well does your classifier work? Let's see:"]}, {"cell_type": "code", "execution_count": null, "id": "bc0dfd55", "metadata": {"colab": {}, "colab_type": "code", "id": "vOC7A_34v1zB"}, "outputs": [], "source": ["# Call the method to establish a baseline\n", "most_common_label, test_accuracy = majority_baseline_accuracy(train_data, test_data)\n", "\n", "print(f'Most common label: {most_common_label}\\n'\n", "      f'Test accuracy:     {test_accuracy:.3f}')"]}, {"cell_type": "markdown", "id": "dc3cdbfe", "metadata": {"colab_type": "text", "id": "dNbq_QvG_XGY"}, "source": ["# To Do: Implement a Naive Bayes classifier\n", "\n", "\n", "## Review of the naive Bayes method\n", "\n", "Recall from lab 1-3 that the Naive Bayes classification method classifies a text $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$ as the class $y_i$ given by the following maximization:\n", "\n", "$$\n", "\\argmax{i} \\Prob(y_i \\given \\vect{w}) \\approx \\argmax{i} \\Prob(y_i) \\cdot \\prod_{j=1}^M \\Prob(w_j \\given y_i)\n", "$$\n", "\n", "or equivalently (since taking the log is monotonic)\n", "\n", "\\begin{align}\n", "\\argmax{i} \\Prob(y_i \\given \\vect{w}) &= \\argmax{i} \\log\\Prob(y_i \\given \\vect{w}) \\\\\n", "&\\approx \\argmax{i} \\left(\\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)\\right)\n", "\\end{align}\n", "\n", "All we need, then, to apply the Naive Bayes classification method is values for the various log probabilities: the priors $\\log\\Prob(y_i)$ and the likelihoods $\\log\\Prob(w_j \\given y_i)$, for each feature (word) $w_j$ and each class $y_i$.\n", "\n", "We can estimate the prior probabilities $\\Prob(y_i)$ by examining the empirical probability in the training set. That is, we estimate \n", "\n", "$$ \\Prob(y_i) \\approx \\frac{\\cnt{y_i}}{\\sum_j \\cnt{y_j}} $$\n", "\n", "We can estimate the likelihood probabilities $\\Prob(w_j \\given y_i)$ similarly by examining the empirical probability in the training set. That is, we estimate \n", "\n", "$$ \\Prob(w_j \\given y_i) \\approx \\frac{\\cnt{w_j, y_i}}{\\sum_{j'} \\cnt{w_{j'}, y_i}} $$\n", "\n", "To allow for cases in which the count $\\cnt{w_j, y_i}$ is zero, we can use a modified estimate incorporating add-$\\delta$ smoothing:\n", "\n", "$$ \\Prob(w_j \\given y_i) \\approx \\frac{\\cnt{w_j, y_i} + \\delta}{\\sum_{j'} \\cnt{w_{j'}, y_i} + \\delta \\cdot V} $$"]}, {"cell_type": "markdown", "id": "f02442b3", "metadata": {"colab_type": "text", "id": "dNbq_QvG_XGY"}, "source": ["## Two conceptions of the naive Bayes method implementation\n", "\n", "We can store all of these parameters in different ways, leading to two different implementation conceptions. We review two conceptions of implementing the naive Bayes classification of a text $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$, corresponding to using different representations of the input $\\vect{x}$ to the model: the index representation and the bag-of-words representation. \n", "\n", "Within each conception, the parameters of the model will be stored in one or more matrices. The conception dictates what operations will be performed with these matrices.\n", "\n", "### Using the index representation\n", "\n", "In the first conception, we take the input elements $\\vect{x} = \\langle x_1, x_2, \\ldots, x_M \\rangle$ to be the _vocabulary indices_ of the words $\\vect{w} = w_1 \\cdots w_M$. That is, each word token $w_i$ is of the word type in the vocabulary $\\vect{v}$ at index $x_i$, so \n", "\n", "$$ v_{x_i} = w_i $$\n", "\n", "In this representation, the input vector has the same length as the word sequence.\n", "\n", "We think of the likelihood probabilities as forming a matrix, call it $\\vect{L}$, where the $i,j$-th element stores $\\log \\Prob(v_j \\given y_i)$. \n", "\n", "$$\\vect{L}_{ij} = \\log\\Prob(v_j \\given y_i)$$\n", "\n", "Similarly, for the priors, we'll have \n", "\n", "$$\\vect{P}_{i} = \\log\\Prob(y_i)$$\n", "\n", "Now the maximization can be implemented as \n", "\n", "\\begin{align}\n", "\\argmax{i} \\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)\n", "&= \\argmax{i} \\vect{P}_i + \\sum_{j=1}^M \\vect{L}_{i, x_j}\n", "\\end{align}\n", "\n", "Implemented in this way, we see that the use of each input $x_i$ is as an _index_ into the likelihood matrix. \n", "\n", "### Using the bag-of-words representation\n", "\n", "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/naive-bayes-figure.png\" width=400 align=right />\n", "\n", "Notice that since each word in the input is treated separately, the order of the words doesn't matter. Rather, all that matters is how frequently each word type occurs in a text. Consequently, we can use the bag-of-words representation introduced in lab 1-1.\n", "\n", "Recall that the bag-of-words representation of a text is just its frequency distribution over the vocabulary, which we will notate $bow(\\vect{w})$. Given a vocabulary of word types $\\vect{v} = \\langle v_1, v_2, \\ldots, v_V \\rangle$, the representation of a sentence $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$ is a vector $\\vect{x}$ of size $V$, where \n", "\n", "$$\\begin{aligned}\n", "bow(\\vect{w})_j &= \\sum_{i=1}^M 1[w_i = v_j] & \\mbox{for $1 \\leq j \\leq V$}\n", "\\end{aligned}$$\n", "\n", "We write $1[w_i = v_j]$ to indicate 1 if $w_i = v_j$ and 0 otherwise. For convenience, we'll add an extra $(V+1)$-st element to the end of the bag-of-words vector, a single $1$ whose use will be clear shortly. That is,\n", "\n", "$$bow(\\vect{w})_{V+1} = 1$$\n", "\n", "Under this conception, then, we'll take the input $\\vect{x}$ to be $bow(\\vect{w})$. Instead of the input having the same length as the text, it has the same length as the vocabulary.\n", "\n", "As described in lecture, represented in this way, the quantity to be maximized in the naive Bayes method\n", "\n", "$$\\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)$$\n", "\n", "can be calculated as \n", "\n", "$$\\log\\Prob(y_i) + \\sum_{j=1}^V x_j \\cdot \\log\\Prob(v_j \\given y_i)$$\n", "\n", "which is just $\\vect{U} \\vect{x}$ for a suitable choice of $N \\times (V+1)$ matrix $\\vect{U}$, namely\n", "\n", "$$ \\vect{U}_{ij} = \\left\\{\n", "    \\begin{array}{ll}\n", "        \\log \\Prob(v_j \\given y_i) & \\mbox{$1 \\leq i \\leq N$ and $1 \\leq j \\leq V$} \\\\\n", "        \\log \\Prob(y_i) & \\mbox{$1 \\leq i \\leq N$ and $j = V+1$} \n", "    \\end{array} \\right.\n", "$$\n", "\n", "Under this implementation conception, we've reduced naive Bayes calculations to a single matrix operation. This conception is depicted in the figure at right.\n", "\n", "You are free to use either conception in your implementation of naive Bayes."]}, {"cell_type": "markdown", "id": "87aad4d7", "metadata": {"colab_type": "text", "id": "bJDEXnaESogl"}, "source": ["## Implement the naive Bayes classifier\n", " \n", "For the implementation, we ask you to implement a Python class `NaiveBayes` that will have (at least) the following three methods:\n", "\n", "1. `__init__`: An initializer that takes `text_vocab`, `label_vocab`, and `pad_index` as inputs.\n", "\n", "2. `train`: A method that takes a training data iterator and estimates all of the log probabilities $\\log\\Prob(y_i)$ and $\\log\\Prob(v_j \\given y_i)$ as described above. Perform add-$\\delta$ smoothing with $\\delta=1$. These parameters will be used by the `evaluate` method to evaluate a test dataset for accuracy, so you'll want to store them in some data structures in objects of the class.\n", "\n", "3. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n", "\n", "You can organize your code using either of the conceptions of Naive Bayes described above.\n", "\n", "You should expect to achieve about an **86% test accuracy** on the ATIS task."]}, {"cell_type": "code", "execution_count": null, "id": "fe521690", "metadata": {"colab": {}, "colab_type": "code", "id": "SwSLwSEO2uyw"}, "outputs": [], "source": ["class NaiveBayes():\n", "  def __init__ (self, text_vocab, label_vocab, pad_index):\n", "    self.pad_index = pad_index\n", "    self.V = len(text_vocab) # vocabulary size\n", "    self.N = len(label_vocab) # the number of classes\n", "    # TODO: Add your code here\n", "    ...\n", "    \n", "  \n", "  def train(self, iterator):\n", "    \"\"\"Calculates and stores log probabilities for training dataset `iterator`.\"\"\"\n", "    # TODO: Implement this method.\n", "    ...\n", "\n", "  def evaluate(self, iterator):\n", "    \"\"\"Returns the model's accuracy on a given dataset `iterator`.\"\"\"\n", "    # TODO: Implement this method.\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "id": "612fa845", "metadata": {"colab": {}, "colab_type": "code", "id": "WOO52qZMv1zS"}, "outputs": [], "source": ["# Instantiate and train classifier\n", "nb_classifier = NaiveBayes(text_vocab, label_vocab, pad_index)\n", "nb_classifier.train(train_iter)\n", "\n", "# Evaluate model performance\n", "print(f'Training accuracy: {nb_classifier.evaluate(train_iter):.3f}\\n'\n", "      f'Test accuracy:     {nb_classifier.evaluate(test_iter):.3f}')"]}, {"cell_type": "markdown", "id": "f7b64b4f", "metadata": {"colab_type": "text", "id": "-pH4ph3uHnvD"}, "source": ["# To Do: Implement a logistic regression classifier\n", "\n", "In this part, you'll complete a PyTorch implementation of a logistic regression (equivalently, a single layer perceptron) classifier. We review logistic regression here highlighting the similarities to the matrix-multiplication conception of naive Bayes. Thus, we take the input $\\vect{x}$ to be the bag-of-words representation $bow(\\vect{w})$. But as before you are free to use either implementation approach.\n", "\n", "## Review of logistic regression\n", "\n", "Similar to naive Bayes, in logistic regression, we assign a probability to a text $\\vect{x}$ by merely multiplying an $N \\times V$ matrix $\\vect{U}$ by it. However, we don't stipulate that the values in the matrix $\\vect{U}$ be estimated from the training corpus in the \"naive Bayes\" manner. Instead, we allow them to take on any value, using a training regime to select good values.\n", "\n", "In order to make sure that the output of the matrix multiplication $\\vect{U}\\vect{x}$ is mapped onto a probability distribution, we apply a nonlinear function to renormalize the values. We use the softmax function, a generalization of the sigmoid function from lab 1-4, defined by \n", "\n", "$$\\softmax(\\vect{z})_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{N} \\exp(z_j)}$$\n", "\n", "for each of the indices $i$ from $1$ to $N$.\n", "\n", "In summary, we model $\\Prob (y \\given \\vect{x})$ as\n", "\n", "$$ \\Prob(y_i \\given \\vect{x}) = \\softmax ( \\vect{U} \\vect{x} )_i $$\n", "\n", "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/logistic-regression-figure.png\" alt=\"logistic regression illustration\" width=\"400\"  align=right />\n", "\n", "The calculation of $\\Prob(y \\given \\vect{x})$ for each text $\\vect{x}$ is referred to as the _forward_ computation. In summary, the forward computation for logistic regression involves a linear calculation ($\\vect{U} \\vect{x}$) followed by a nonlinear calculation ($\\softmax$). We think of the perceptron (and more generally many of these neural network models) as transforming from one representation to another. A perceptron performs a linear transformation from the index or bag-of-words representation of the text to a representation as a vector, followed by a nonlinear transformation, a softmax or sigmoid, giving a representation as a probability distribution over the class labels. This single-layer perceptron thus involves two _sublayers_. (In the next part of the project segment, you'll experiment with a multilayer perceptron, with two perceptron layers, and hence four sublayers.)\n", "\n", "The loss function you'll use is the negative log probability $-\\log \\Prob (y \\given \\vect{x})$. The negative is used, since it is convention to minimize loss, whereas we want to maximize log likelihood. \n", "\n", "The forward and loss computations are illustrated in the figure at right. In practice, for numerical stability reasons, PyTorch absorbs the softmax operation into the loss function `nn.CrossEntropyLoss`. That is, the input to the `nn.CrossEntropyLoss` function is the vector of sums $\\vect{U} \\vect{x}$ (the last step in the box marked \"your job\" in the figure) rather than the vector of probabilities $\\Prob(y \\given \\vect{x})$. That makes things easier for you (!), since you're responsible only for the first sublayer.\n", "\n", "Given a forward computation, the weights can then be adjusted by taking a step opposite to the gradient of the loss function. Adjusting the weights in this way is referred to as the _backward_ computation. Fortunately, `torch` takes care of the backward computation for you, just as in lab 1-5.\n", "\n", "The optimization process of performing the forward computation, calculating the loss, and performing the backward computation to improve the weights is done repeatedly until the process converges on a (hopefully) good set of weights. You'll find this optimization process in the `train_all` method that we've provided. The trained weights can then be used to perform classification on a test set. See the `evaluate` method."]}, {"cell_type": "markdown", "id": "5557c865", "metadata": {"colab_type": "text", "id": "KEgCwQVHrVw0"}, "source": ["## Implement the logistic regression classifier\n", "\n", "For the implementation, we ask you to implement a logistic regression classifier as a subclass of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module). You need to implement the following methods:\n", "\n", "1. `__init__`: an initializer that takes `text_vocab`, `label_vocab`, and `pad_index` as inputs.\n", "\n", "    During initialization, you'll want to define a [tensor](https://pytorch.org/docs/stable/tensors.html#torch-tensor) of weights, wrapped in [`torch.nn.Parameter`](https://pytorch.org/docs/master/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter), [initialized randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_), which plays the role of $\\vect{U}$. The elements of this tensor are the parameters of the `torch.nn` instance in the following special technical sense: It is the parameters of the module whose gradients will be calculated and whose values will be updated. Alternatively, **you might find it easier** to use the [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) which is a wrapper to the weight tensor with a lookup implementation.\n", "\n", "2. `forward`: given a text batch of size `batch_size X max_length`, return a tensor of logits of size `batch_size X num_labels`. That is, for each text $\\vect{x}$ in the batch and each label $y$, you'll be calculating $\\vect{U}\\vect{x}$ as shown in the figure, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you won't need to deal with that.\n", "\n", "3. `train_all`: A method that performs training. You might find lab 1-5 useful.\n", "\n", "4. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n", "\n", "Some things to consider:\n", "\n", "1. The parameters of the model, the weights, need to be initialized properly. We suggest initializing them to some small random values. See [`torch.uniform_`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_).\n", "\n", "2. You'll want to make sure that padding tokens are handled properly. What should the weight be for the padding token?\n", "\n", "3. In extracting the proper weights to sum up, based on the word types in a sentence, we are essentially doing a lookup operation. You might find [`nn.Embedding`](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) or [`torch.gather`](https://pytorch.org/docs/stable/generated/torch.gather.html#torch-gather) useful.\n", "\n", "You should expect to achieve about **90%** accuracy on the ATIS classificiation task. "]}, {"cell_type": "code", "execution_count": null, "id": "5e7e8b7b", "metadata": {"colab": {}, "colab_type": "code", "id": "dBdFcvg-PYBo"}, "outputs": [], "source": ["class LogisticRegression(nn.Module):\n", "  def __init__ (self, text_vocab, label_vocab, pad_index):\n", "    super().__init__()\n", "    self.pad_index = pad_index\n", "    # Keep the vocabulary sizes available\n", "    self.N = len(label_vocab) # num_classes\n", "    self.V = len(text_vocab)  # vocab_size\n", "    # Specify cross-entropy loss for optimization\n", "    self.criterion = nn.CrossEntropyLoss()\n", "    # TODO: Create and initialize a tensor for the weights,\n", "    #       or create an nn.Embedding module and initialize\n", "    ...\n", "\n", "  def forward(self, text_batch):\n", "    # TODO: Calculate the logits (Ux) for the `text_batch`, \n", "    #       returning a tensor of size batch_size x num_labels\n", "    ...\n", "\n", "  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n", "    # Switch the module to training mode\n", "    self.train()\n", "    # Use Adam to optimize the parameters\n", "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n", "    best_validation_accuracy = -float('inf')\n", "    best_model = None\n", "    # Run the optimization for multiple epochs\n", "    with tqdm(range(epochs), desc='train', position=0) as pbar:\n", "      for epoch in pbar:\n", "        c_num = 0\n", "        total = 0\n", "        running_loss = 0.0\n", "\n", "        for batch in tqdm(train_iter, desc='batch', leave=False):\n", "          # TODO: set labels, compute logits (Ux in this model), \n", "          #       loss, and update parameters\n", "          ...\n", "          labels = ...\n", "          logits = ...\n", "          loss = ...\n", "          ...\n", "          # Prepare to compute the accuracy\n", "          predictions = torch.argmax(logits, dim=1)\n", "          total += predictions.size(0)\n", "          c_num += (predictions == labels).float().sum().item()        \n", "          running_loss += loss.item() * predictions.size(0)\n", "\n", "        # Evaluate and track improvements on the validation dataset\n", "        validation_accuracy = self.evaluate(val_iter)\n", "        if validation_accuracy > best_validation_accuracy:\n", "          best_validation_accuracy = validation_accuracy\n", "          self.best_model = copy.deepcopy(self.state_dict())\n", "        epoch_loss = running_loss / total\n", "        epoch_acc = c_num / total\n", "        pbar.set_postfix(epoch=epoch+1, loss=epoch_loss, train_acc = epoch_acc, val_acc=validation_accuracy)\n", "\n", "  def evaluate(self, iterator):\n", "    \"\"\"Returns the model's accuracy on a given dataset `iterator`.\"\"\"\n", "    self.eval()   # switch the module to evaluation mode\n", "    # TODO: Compute accuracy\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "id": "bd3f32cb", "metadata": {"colab": {}, "colab_type": "code", "id": "6wW6UcFqv1zp"}, "outputs": [], "source": ["# Instantiate the logistic regression classifier and run it\n", "model = LogisticRegression(text_vocab, label_vocab, pad_index).to(device) \n", "model.train_all(train_iter, val_iter)\n", "model.load_state_dict(model.best_model)\n", "test_accuracy = model.evaluate(test_iter)\n", "print (f'Test accuracy: {test_accuracy:.4f}')"]}, {"cell_type": "markdown", "id": "30015b49", "metadata": {"colab_type": "text", "id": "Te35cWGOJlf9"}, "source": ["# To Do: Implement a multilayer perceptron\n", "\n", "## Review of multilayer perceptrons\n", "\n", "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/multilayer-perceptron-figure.png\" alt=\"multilayer perceptron illustration\" width=\"400\"  align=right />\n", "\n", "In the last part, you implemented a perceptron, a model that involved a linear calculation (the sum of weights) followed by a nonlinear calculation (the softmax, which converts the summed weight values to probabilities). In a multi-layer perceptron, we take the output of the first perceptron to be the input of a second perceptron (and of course, we could continue on with a third or even more).\n", "\n", "In this part, you'll implement the forward calculation of a two-layer perceptron, again letting PyTorch handle the backward calculation as well as the optimization of parameters. The first layer will involve a linear summation as before and a **sigmoid** as the nonlinear function. The second will involve a linear summation and a softmax (the latter absorbed, as before, into the loss function). Thus, the difference from the logistic regression implementation is simply the adding of the sigmoid and second linear calculations. See the figure for the structure of the computation. \n", "\n"]}, {"cell_type": "markdown", "id": "ac2ed538", "metadata": {"colab_type": "text", "id": "FsBnCFe0CnUv"}, "source": ["## Implement a multilayer perceptron classifier\n", "\n", "For the implementation, we ask you to implement a two layer perceptron classifier, again as a subclass of the [`torch.nn` module](https://pytorch.org/docs/stable/nn.html). You might reuse quite a lot of the code from logistic regression. As before, you need to implement the following methods:\n", "\n", "1. `__init__`: An initializer that takes `text_vocab`, `label_vocab`, `pad_index`, and `hidden_size` specifying the size of the hidden layer (e.g., in the above illustration, `hidden_size` is `D`).\n", "\n", "    During initialization, you'll want to define two tensors of weights, which serve as the parameters of this model, one for each layer. You'll want to [initialize them randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_). \n", "    \n", "    The weights in the first layer are a kind of lookup (as in the previous part), mapping words to a vector of size `hidden_size`. The [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) is a good way to set up and make use of this weight tensor.\n", "    \n", "    The weights in the second layer define a linear mapping from vectors of size `hidden_size` to vectors of size `num_labels`. The [`nn.Linear` module](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) or [`torch.mm`](https://pytorch.org/docs/master/generated/torch.mm.html) for matrix multiplication may be helpful here.\n", "\n", "2. `forward`: Given a text batch of size `batch_size X max_length`, the `forward` function returns a tensor of logits of size `batch_size X num_labels`. \n", "\n", "    That is, for each text $\\vect{x}$ in the batch and each label $c$, you'll be calculating $MLP(bow(\\vect{x}))$ as shown in the illustration above, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you don't need to worry about that.\n", "    \n", "    For the sigmoid sublayer, you might find [`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) useful.\n", "    \n", "3. `train_all`: A method that performs training. You might find lab 1-5 useful.\n", "\n", "4. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n", "\n", "You should expect to achieve at least **90%** accuracy on the ATIS classificiation task. "]}, {"cell_type": "code", "execution_count": null, "id": "ce3b3c9d", "metadata": {"colab": {}, "colab_type": "code", "id": "I5TVcZ879gI8"}, "outputs": [], "source": ["class MultiLayerPerceptron(nn.Module):\n", "  def __init__ (self, text_vocab, label_vocab, pad_index, hidden_size=128): \n", "    super().__init__ ()\n", "    self.pad_index = pad_index\n", "    self.hidden_size = hidden_size\n", "    # Keep the vocabulary sizes available\n", "    self.N = len(label_vocab) # num_classes\n", "    self.V = len(text_vocab)  # vocab_size\n", "    # Specify cross-entropy loss for optimization\n", "    self.criterion = nn.CrossEntropyLoss()\n", "    # TODO: Create and initialize neural modules\n", "    ...\n", "\n", "  def forward(self, text_batch):\n", "    # TODO: Calculate the logits for the `text_batch`, \n", "    #       returning a tensor of size batch_size x num_labels\n", "    ...\n", "  \n", "  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n", "    # Switch the module to training mode\n", "    self.train()\n", "    # Use Adam to optimize the parameters\n", "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n", "    best_validation_accuracy = -float('inf')\n", "    best_model = None\n", "    # Run the optimization for multiple epochs\n", "    with tqdm(range(epochs), desc='train', position=0) as pbar:\n", "      for epoch in pbar:\n", "        c_num = 0\n", "        total = 0\n", "        running_loss = 0.0\n", "        for batch in tqdm(train_iter, desc='batch', leave=False):\n", "          # TODO: set labels, compute logits (Ux in this model), \n", "          #       loss, and update parameters\n", "          ...\n", "          labels = ...\n", "          logits = ...\n", "          loss = ...\n", "          ...\n", "          # Prepare to compute the accuracy\n", "          predictions = torch.argmax(logits, dim=1)\n", "          total += predictions.size(0)\n", "          c_num += (predictions == labels).float().sum().item()        \n", "          running_loss += loss.item() * predictions.size(0)\n", "\n", "        # Evaluate and track improvements on the validation dataset\n", "        validation_accuracy = self.evaluate(val_iter)\n", "        if validation_accuracy > best_validation_accuracy:\n", "          best_validation_accuracy = validation_accuracy\n", "          self.best_model = copy.deepcopy(self.state_dict())\n", "        epoch_loss = running_loss / total\n", "        epoch_acc = c_num / total\n", "        pbar.set_postfix(epoch=epoch+1, loss=epoch_loss, train_acc = epoch_acc, val_acc=validation_accuracy)\n", "\n", "  def evaluate(self, iterator):\n", "    \"\"\"Returns the model's accuracy on a given dataset `iterator`.\"\"\"\n", "    # TODO: Compute accuracy\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "id": "025db464", "metadata": {"colab": {}, "colab_type": "code", "id": "F4Bs0f7Hv1z4"}, "outputs": [], "source": ["# Instantiate classifier and run it\n", "model = MultiLayerPerceptron(text_vocab, label_vocab, pad_index, hidden_size=128).to(device) \n", "model.train_all(train_iter, val_iter)\n", "model.load_state_dict(model.best_model)\n", "test_accuracy = model.evaluate(test_iter)\n", "print (f'Test accuracy: {test_accuracy:.4f}')"]}, {"cell_type": "markdown", "id": "9193bd65", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "# Lessons learned\n", "\n", "Take a look at some of the examples that were classified correctly and incorrectly by your best method.\n", "\n", "**Question:** Do you notice anything about the incorrectly classified examples that might indicate _why_ they were classified incorrectly?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_lessons\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "ab01d770", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "id": "49170382", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->"]}, {"cell_type": "code", "execution_count": null, "id": "c4eeb544", "metadata": {}, "outputs": [], "source": ["..."]}, {"cell_type": "markdown", "id": "c4cc72b9", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "# Debrief\n", "\n", "**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on include the following: \n", "\n", "* Was the project segment clear or unclear? Which portions?\n", "* Were the readings appropriate background for the project segment? \n", "* Are there additions or changes you think would make the project segment better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "4b81f183", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"attachments": {}, "cell_type": "markdown", "id": "a262220b", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# Instructions for submission of the project segment\n", "\n", "This project segment should be submitted to Gradescope at <https://rebrand.ly/project1-submit-code> and <https://rebrand.ly/project1-submit-pdf>, which will be made available some time before the due date.\n", "\n", "Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion. You should submit your code to Gradescope at the code submission assignment at <https://rebrand.ly/project1-submit-code>.\n", "\n", "We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope at <https://rebrand.ly/project1-submit-pdf>."]}, {"attachments": {}, "cell_type": "markdown", "id": "cc96d77a", "metadata": {}, "source": ["# End of project segment 1"]}], "metadata": {"accelerator": "GPU", "celltoolbar": "Edit Metadata", "colab": {"collapsed_sections": [], "include_colab_link": true, "name": "project1_classification.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "otter-latest", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}, "title": "CS187 Project Segment 1: Text Classification", "vscode": {"interpreter": {"hash": "4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"}}}, "nbformat": 4, "nbformat_minor": 5}